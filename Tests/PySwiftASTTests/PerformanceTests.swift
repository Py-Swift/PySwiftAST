import XCTest
@testable import PySwiftAST
import PySwiftCodeGen
import Foundation

/// Performance benchmarks to track parser optimization progress
/// Goal: Achieve >2x speedup over Python's ast module through optimization
///
/// ‚ö†Ô∏è IMPORTANT: These tests MUST be run in RELEASE mode for accurate results!
///    Debug builds are 3-4x slower and will show "regressions"
///
/// Run with: swift test -c release --filter PerformanceTests
///
final class PerformanceTests: XCTestCase {
    
    // MARK: - Test Configuration
    
    /// Number of iterations for each benchmark
    let iterations = 100
    
    /// Performance targets relative to Python ast module baseline
    struct PerformanceTargets {
        static let parsingSpeedup: Double = 1.35  // Current: 1.35x, Goal: 2.0x+
        static let roundtripSpeedup: Double = 1.04  // Current: 1.04x, Goal: 1.5x+
        
        // Baseline from Python ast module (measured on macOS, 2023 hardware)
        static let pythonParsingMedian: Double = 0.00867  // 8.67ms in seconds
        static let pythonRoundtripMedian: Double = 0.03023  // 30.23ms in seconds
        
        // Check if running in release mode
        static var isReleaseMode: Bool {
            #if DEBUG
            return false
            #else
            return true
            #endif
        }
    }
    
    // MARK: - Parsing Performance Tests
    
    func testParsingPerformance_Django() throws {
        let djangoPath = try getTestFilePath("django_query.py")
        let source = try String(contentsOfFile: djangoPath)
        
        // Check build configuration
        if !PerformanceTargets.isReleaseMode {
            print("\n‚ö†Ô∏è  WARNING: Running in DEBUG mode!")
            print("   Performance tests should be run with: swift test -c release")
            print("   Skipping strict performance checks...\n")
        }
        
        print("\n" + String(repeating: "=", count: 70))
        print("PARSING PERFORMANCE TEST - Django query.py")
        print("Build mode: \(PerformanceTargets.isReleaseMode ? "RELEASE ‚úÖ" : "DEBUG ‚ö†Ô∏è")")
        print("File size: \(source.count) bytes, \(source.split(separator: "\n").count) lines")
        print(String(repeating: "=", count: 70))
        
        // Tokenize once (not part of parsing benchmark)
        let tokenizer = Tokenizer(source: source)
        let tokens = try tokenizer.tokenize()
        
        // Warmup
        for _ in 0..<10 {
            _ = try Parser(tokens: tokens).parse()
        }
        
        // Benchmark
        var times: [TimeInterval] = []
        for _ in 0..<iterations {
            let start = Date()
            _ = try Parser(tokens: tokens).parse()
            let end = Date()
            times.append(end.timeIntervalSince(start))
        }
        
        let stats = calculateStats(times)
        printStats("PySwiftAST Parsing", stats)
        
        // Compare against Python baseline
        let speedup = PerformanceTargets.pythonParsingMedian / stats.median
        let targetSpeedup = PerformanceTargets.parsingSpeedup
        
        print("\nPerformance vs Python ast.parse():")
        print("  Current speedup: \(String(format: "%.2f", speedup))x")
        print("  Target speedup:  \(String(format: "%.2f", targetSpeedup))x")
        
        if speedup >= 2.0 {
            print("  ‚úÖ EXCELLENT: 2x+ speedup achieved!")
        } else if speedup >= targetSpeedup {
            print("  ‚úÖ PASS: Meeting current target")
            print("  üéØ Room for optimization: Target is 2.0x+")
        } else {
            print("  ‚ö†Ô∏è  REGRESSION: Below baseline performance")
        }
        
        print(String(repeating: "=", count: 70) + "\n")
        
        // Only enforce strict performance checks in release mode
        if PerformanceTargets.isReleaseMode && speedup < targetSpeedup * 0.8 {
            XCTFail("Performance regression: \(speedup)x is below 80% of target \(targetSpeedup)x")
        }
    }
    
    // MARK: - Round-trip Performance Tests
    
    func testRoundtripPerformance_Django() throws {
        let djangoPath = try getTestFilePath("django_query.py")
        let source = try String(contentsOfFile: djangoPath)
        
        // Check build configuration
        if !PerformanceTargets.isReleaseMode {
            print("\n‚ö†Ô∏è  WARNING: Running in DEBUG mode!")
            print("   Skipping strict performance checks...\n")
        }
        
        print("\n" + String(repeating: "=", count: 70))
        print("ROUND-TRIP PERFORMANCE TEST - Django query.py")
        print("Build mode: \(PerformanceTargets.isReleaseMode ? "RELEASE ‚úÖ" : "DEBUG ‚ö†Ô∏è")")
        print("Operation: parse ‚Üí generate ‚Üí reparse")
        print(String(repeating: "=", count: 70))
        
        // Tokenize once
        let tokenizer = Tokenizer(source: source)
        let tokens = try tokenizer.tokenize()
        
        // Warmup
        for _ in 0..<10 {
            let module = try Parser(tokens: tokens).parse()
            let generated = generatePythonCode(from: module)
            let generatedTokens = try Tokenizer(source: generated).tokenize()
            _ = try Parser(tokens: generatedTokens).parse()
        }
        
        // Benchmark
        var times: [TimeInterval] = []
        for _ in 0..<iterations {
            let start = Date()
            let module = try Parser(tokens: tokens).parse()
            let generated = generatePythonCode(from: module)
            let generatedTokens = try Tokenizer(source: generated).tokenize()
            _ = try Parser(tokens: generatedTokens).parse()
            let end = Date()
            times.append(end.timeIntervalSince(start))
        }
        
        let stats = calculateStats(times)
        printStats("PySwiftAST Round-trip", stats)
        
        // Compare against Python baseline
        let speedup = PerformanceTargets.pythonRoundtripMedian / stats.median
        let targetSpeedup = PerformanceTargets.roundtripSpeedup
        
        print("\nPerformance vs Python (parse + unparse + reparse):")
        print("  Current speedup: \(String(format: "%.2f", speedup))x")
        print("  Target speedup:  \(String(format: "%.2f", targetSpeedup))x")
        print("  Goal speedup:    1.50x+")
        
        if speedup >= 1.5 {
            print("  ‚úÖ EXCELLENT: 1.5x+ speedup achieved!")
        } else if speedup >= targetSpeedup {
            print("  ‚úÖ PASS: Meeting current target")
            print("  üéØ Room for optimization: Target is 1.5x+")
        } else {
            print("  ‚ö†Ô∏è  REGRESSION: Below baseline performance")
        }
        
        print(String(repeating: "=", count: 70) + "\n")
        
        // Only enforce strict performance checks in release mode
        if PerformanceTargets.isReleaseMode && speedup < targetSpeedup * 0.8 {
            XCTFail("Performance regression: \(speedup)x is below 80% of target \(targetSpeedup)x")
        }
    }
    
    // MARK: - Component Performance Tests
    
    func testTokenizationPerformance() throws {
        let djangoPath = try getTestFilePath("django_query.py")
        let source = try String(contentsOfFile: djangoPath)
        
        print("\n" + String(repeating: "=", count: 70))
        print("TOKENIZATION PERFORMANCE TEST")
        print(String(repeating: "=", count: 70))
        
        // Warmup
        for _ in 0..<10 {
            _ = try Tokenizer(source: source).tokenize()
        }
        
        // Benchmark
        var times: [TimeInterval] = []
        for _ in 0..<iterations {
            let start = Date()
            _ = try Tokenizer(source: source).tokenize()
            let end = Date()
            times.append(end.timeIntervalSince(start))
        }
        
        let stats = calculateStats(times)
        printStats("Tokenization", stats)
        
        print("\nüí° Optimization opportunities:")
        print("   - Character processing (UTF-8 vs UTF-16)")
        print("   - String scanning efficiency")
        print("   - Token array allocation strategies")
        print(String(repeating: "=", count: 70) + "\n")
    }
    
    func testCodeGenerationPerformance() throws {
        let djangoPath = try getTestFilePath("django_query.py")
        let source = try String(contentsOfFile: djangoPath)
        
        print("\n" + String(repeating: "=", count: 70))
        print("CODE GENERATION PERFORMANCE TEST")
        print(String(repeating: "=", count: 70))
        
        // Parse once
        let tokenizer = Tokenizer(source: source)
        let tokens = try tokenizer.tokenize()
        let module = try Parser(tokens: tokens).parse()
        
        // Warmup
        for _ in 0..<10 {
            _ = generatePythonCode(from: module)
        }
        
        // Benchmark
        var times: [TimeInterval] = []
        for _ in 0..<iterations {
            let start = Date()
            _ = generatePythonCode(from: module)
            let end = Date()
            times.append(end.timeIntervalSince(start))
        }
        
        let stats = calculateStats(times)
        printStats("Code Generation", stats)
        
        print("\nüí° Optimization opportunities:")
        print("   - String concatenation vs String builder")
        print("   - Indentation caching")
        print("   - Reduce allocations in hot paths")
        print(String(repeating: "=", count: 70) + "\n")
    }
    
    // MARK: - Utilities
    
    struct Stats {
        let min: TimeInterval
        let median: TimeInterval
        let mean: TimeInterval
        let p95: TimeInterval
        let p99: TimeInterval
        let max: TimeInterval
    }
    
    func calculateStats(_ times: [TimeInterval]) -> Stats {
        let sorted = times.sorted()
        let count = sorted.count
        
        return Stats(
            min: sorted.first!,
            median: sorted[count / 2],
            mean: sorted.reduce(0, +) / Double(count),
            p95: sorted[Int(Double(count) * 0.95)],
            p99: sorted[Int(Double(count) * 0.99)],
            max: sorted.last!
        )
    }
    
    func printStats(_ label: String, _ stats: Stats) {
        print("\n\(label) (\(iterations) iterations):")
        print("  Min:    \(String(format: "%8.3f", stats.min * 1000)) ms")
        print("  Median: \(String(format: "%8.3f", stats.median * 1000)) ms")
        print("  Mean:   \(String(format: "%8.3f", stats.mean * 1000)) ms")
        print("  P95:    \(String(format: "%8.3f", stats.p95 * 1000)) ms")
        print("  P99:    \(String(format: "%8.3f", stats.p99 * 1000)) ms")
        print("  Max:    \(String(format: "%8.3f", stats.max * 1000)) ms")
    }
    
    func getTestFilePath(_ filename: String) throws -> String {
        // Use Bundle to find the resource in the test bundle
        guard let resourceURL = Bundle.module.url(
            forResource: filename.replacingOccurrences(of: ".py", with: ""),
            withExtension: "py",
            subdirectory: "test_files"
        ) else {
            throw NSError(domain: "TestError", code: 1,
                         userInfo: [NSLocalizedDescriptionKey: "Test file not found: \(filename)"])
        }
        
        return resourceURL.path
    }
}

// MARK: - Performance Optimization TODOs

/*
 OPTIMIZATION ROADMAP - Target: 2x+ speedup over Python
 
 1. PARSING OPTIMIZATIONS:
    ‚òê Reduce bounds checking overhead
    ‚òê Use unsafe buffer pointers for hot paths
    ‚òê Optimize token lookahead (current: O(1) but could be faster)
    ‚òê Reduce AST node allocations
    ‚òê Pool commonly used node types
    ‚òê Optimize expression parsing precedence chain
    ‚òê Inline hot path functions
 
 2. TOKENIZATION OPTIMIZATIONS:
    ‚òê UTF-8 string handling (avoid UTF-16 conversion overhead)
    ‚òê Reduce string allocations during scanning
    ‚òê Optimize number parsing
    ‚òê Fast path for common tokens (keywords, operators)
    ‚òê Avoid redundant character lookups
 
 3. CODE GENERATION OPTIMIZATIONS:
    ‚òê Use string builders instead of concatenation
    ‚òê Cache indentation strings
    ‚òê Reduce protocol witness table lookups
    ‚òê Batch string writing
 
 4. MEMORY OPTIMIZATIONS:
    ‚òê Reduce AST node size (pack fields)
    ‚òê Use value types where possible
    ‚òê Arena allocation for AST nodes
    ‚òê Reduce ARC overhead
 
 5. COMPILER OPTIMIZATIONS:
    ‚òê Profile-guided optimization
    ‚òê Link-time optimization (already using -c release)
    ‚òê Whole-module optimization
    ‚òê Function specialization
 
 Current Status:
 - Parsing: 1.35x faster than Python (Goal: 2.0x+)
 - Round-trip: 1.04x faster than Python (Goal: 1.5x+)
 
 Next Steps:
 1. Profile with Instruments to find hotspots
 2. Implement low-hanging fruit optimizations
 3. Measure impact and iterate
 */
